<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[java对象转型]]></title>
      <url>%2F2017%2F05%2F08%2Fjava%2F2017-05-08%2F</url>
      <content type="text"><![CDATA[java对象转型 向上转型和向下转型 什么是对象转型?对象转型是在继承的基础上而言的，对象类型转换主要包含向上转型和向下转型。继承是java面向对象的基本特征之一，是代码复用的一种机制，通过继承子类可以复用父类的方法等；如果父类的方法不满足子类的需求，子类可以通过重写父类的方法或者新增属于自己特性的方法对自身的功能加以扩展。 1向上转型：子类引用的对象转换为父类类型称为向上转型。子类的对象转为父类对象，此处父类对象可以是接口。{eg：Annimal dog =new Dog();} 2向下转型：父类引用的对象转换为子类类型称为向下转型。{eg：Dog dog =new Animal();//不安全的向下转型上述编译通过，但是运行时候报错，类型转换错误！正确操作：Annimal ad =new Dog()；Dog dog = （Dog）ad；} 向上转型注意： 前者是一个向上转型，Animal dog 引用指向new Dog();子类对象当成父类对象，只能调用父类的成员，如果子类重写了父类的方法就根据这个引用指向调用子类重写的这个方法（这个方法就是覆盖override）。这个调用过程就称为“动态绑定”。（即子类重写父类方法，向上转型对象调用该方法，方法指向子类中重写的方法） 向上转型时，父类指向子类引用对象会遗失除与父类对象共有的其他方法，也就是在转型过程中，子类的新有的方法都会遗失掉，在编译时，系统会提供找不到方法的错误。如： 12345678910111213141516171819 public class Animal &#123; public void eat()&#123; System.out.println(&quot;animal eatting...&quot;); &#125;&#125;class Bird extends Animal&#123; public void eat()&#123; System.out.println(&quot;bird eatting...&quot;); &#125; public void fly()&#123; System.out.println(&quot;bird flying...&quot;); &#125;&#125;class Main&#123; public static void main(String[] args) &#123; Animal b=new Bird(); //向上转型 b.eat(); b.fly(); //此处提示在Animal中没有定义fly方法。｝ 向下转型注意：情况一：如果父类引用的对象如果引用的是指向的子类对象（即先向上转型），那么在向下转型的过程中是安全的。也就是编译是不会出错误的。此时对象可以调用子类所有的成员。情况二：如果父类引用的对象是父类本身，那么在向下转型的过程中是不安全的，编译不会出错，但是运行时会出现java.lang.ClassCastException错误。它可以使用instanceof来避免出错此类错误。 12345678910111213 class Main&#123; public static void main(String[] args) &#123; Animal b=new Bird(); //先向上转型（父类引用对象指向子类对象） Bird bird = (Bird) b;//向下转型，编译和运行都不报错 Animal a=new Animal(); //（父类引用对象指向父类本身对象） Bird bird = (Bird) a;//不安全的向下转型，编译正常，运行报错（类型转换错误） //解决方法 instance of if(a instanceof Bird)&#123; Bird bird=(Bird) a; bird.eat(); bird.fly(); &#125;｝ 总结：1、父类引用可以指向子类对象，子类引用不能指向父类对象。2、把子类对象直接赋给父类引用叫upcasting向上转型，向上转型不用强制转型。3、把指向子类对象的父类引用赋给子类引用叫向下转型（downcasting），要强制转型。4、upcasting 会丢失子类特有的方法,但是子类overriding 父类的方法，子类方法有效 。5、向上转型的作用，减少重复代码，父类为参数，调有时用子类作为参数，就是利用了向上转型。这样使代码变得简洁。体现了JAVA的抽象编程思想。 参考:夜雨阑珊]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Let Your Heart Hold Fast]]></title>
      <url>%2F2017%2F04%2F16%2Fmusic%2F2017-04-16%2F</url>
      <content type="text"><![CDATA[Let Your Heart Hold FastAll my days are spent 那飞速逝去的时日All my cards are dealt那已经确定的命运Oh ,the descolation grows悲凉无尽蔓延心脏Every inch revealed心脏每一寸As my heart is pierced都被刺穿Oh,my soul is now exposed灵魂彻底暴露In the ocean deep穿过深邃的海洋In the canyons steep越过险峻的峡谷Walls of granite here I stand我立在岩石之上All my desperate calls我所有绝望的呼唤Rcho off the walls在墙间回荡Back and forth;then back again久久不息To believe I walk alone相信自己能踽踽独行Is a lie that I’ve been told这是别人告诉我的谎言So let your heart hold fast所以不要灰心丧气For this soon shall pass因为一切都将成为过去Like the high tide takes the sand就像沙子会被海潮带走Oh,Oh,Oh,Oh,！哦~Oh,Oh,Oh,Oh,！哦~Oh,Oh,Oh,Oh,Oh！哦~Oh,Oh,Oh,Oh,！哦~Oh,Oh,Oh,Oh,！哦~Oh,Oh,Oh,Oh,Oh！哦~At the bitter end在痛苦的最后Salt and liquid blend泪水涌出From the corner of eyes从我的眼角滑落All the miles wrecked蹒跚走过的所有路Every broken step艰难走过的每一步Always searching,always blind一直在盲目的寻找Never fear!No!Never fear!不要畏惧！不要畏惧！Never fear!No! Never fear!永不畏惧！永不畏惧！So let your heart hold fast所以不要灰心丧气For this soon shall pass因为一切将成为过去There’s another hill ahead前方将是人生的另一座高峰&gt; 这世上总有很多我们无法掌控的事情，就像我想你一样，总是那么的不经意间。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[flask get/post参数获取]]></title>
      <url>%2F2017%2F03%2F22%2Fpython%2Fflask%2F2017-03-22%2F</url>
      <content type="text"><![CDATA[flask 使用笔记获得get和post 提交的参数post 提交参数获取request.form[‘xxx’] 其类型为：ImmutableMultiDict123456@app.route(&quot;/deleteInfo&quot;, methods=[&apos;post&apos;])@allow_cross_domaindef delete_info(): print(type(request.form)) # &lt; class &apos;werkzeug.datastructures.ImmutableMultiDict&apos;&gt; _id = request.form[&quot;_id&quot;] get 提交参数获取1234@app.route(&apos;/queryDetl&apos;)@allow_cross_domaindef query_detail(): _id = request.args.get(&quot;_id&quot;)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[解决Flask 跨域请求问题]]></title>
      <url>%2F2017%2F03%2F21%2Fpython%2Fflask%2F2017-03-21%2F</url>
      <content type="text"><![CDATA[python flask 跨域访问装饰器实现现在大多数的web项目都已经是前后端分离；一般纯的api接口需要考虑跨域访问问题 下面是简单的跨域访问装饰器在flask中的实现（其他语言web项目跨域处理方法类似）12345678910111213141516171819from functools import wrapsfrom flask import make_responsedef allow_cross_domain(fun): @wraps(fun) def wrapper_fun(*args, **kwargs): rst = make_response(fun(*args, **kwargs)) rst.headers[&apos;Access-Control-Allow-Origin&apos;] = &apos;*&apos; rst.headers[&apos;Access-Control-Allow-Methods&apos;] = &apos;PUT,GET,POST,DELETE&apos; allow_headers = &quot;Referer,Accept,Origin,User-Agent&quot; rst.headers[&apos;Access-Control-Allow-Headers&apos;] = allow_headers return rst return wrapper_fun@app.route(&apos;/hosts/&apos;)@allow_cross_domaindef domains(): pass]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[my first hexo]]></title>
      <url>%2F2017%2F03%2F20%2Fmy-first-hexo%2F</url>
      <content type="text"><![CDATA[hello glinlf and hexo!]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Hello World]]></title>
      <url>%2F2017%2F03%2F19%2Fhello-world%2F</url>
      <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[如何使用grequests 异步IO批量请求]]></title>
      <url>%2F2017%2F03%2F18%2Fspider%2F2017-03-18%2F</url>
      <content type="text"><![CDATA[python grequests 库的使用记录 简介： 对于大名鼎鼎的request库相信大家都已经非常熟悉了，不过他有个不好的缺点就是阻塞性IO，在你使用request.get()获取所需要的网页时，除非这个网页下载完成，不然不能进行对新网页的获取；而cup的处理速度远比IO读写的速度快多了，为了充分利用cpu，加快网页爬取速度，这里讲介绍使用异步IO获取网页的grequests库的使用。这两个库都是 kennethreitz开发的，用法非常相似。 安装installpip install grequests 基础用法 12345678910111213141516171819#要爬取的url集合urls = [ url = &apos;https://www.zhihu.com/people/excited-vczh/following&apos;, url = &apos;https://www.zhihu.com/people/liaoxuefeng/following&apos;, url = &apos;https://www.zhihu.com/people/hellolin-30/following&apos;]#同时将上述请求发送rs = (grequests.get(u) for u in urls)#查看发送请求的响应状态码和响应结果： respond_html=[] for resp in grequests.map(rs, exception_handler=exception_handler): # grequests.map可以查看异步io么一个请求返回的状态码200 500 # print(&apos;status:&#123;0&#125; url:&#123;1&#125;&apos;.format(resp, resp.url)) if resp is not None: respond_html.append(resp.text) else: continue 异常的处理 12def exception_handler(request, exception): print(&apos;got exception request: &#123;0&#125;, exception &#123;1&#125;&apos;.format(request, exception)) 异步IO实际运用效果的反思： a.原本毕设在做一个基于python的知乎用户信息特征爬虫系统的，为了加快爬取速度思考使用异步IO，主要思路： 1 对用户信息的的请求入口使用异步IO（grequests）请求多个url地址 2 每一个用户关注人又有多个分页，对分页进行异步IO请求。 原先未使用grequests库，由于阻塞IO的影响，每次只能爬取一个用户信息，再爬取每一用户第一页下的关注人url地址（实际未爬全所有该用户关注的用户）；速度爬取相对较快；使用grequests后去爬取某个用户所有分页下的关注人url后，速度开始变慢，主要原因是：在爬取这个用户后，接着要全部爬完和解析所有分页的html（基本爬全所有关注的用户）；比如某个用户关注上万人，系统爬取该用户下所有关注人都要耗费大把时间。所以异步IO提速还需要看实际的运用情况和环境。比如本身网络带宽限制，机器硬件限制等等都会影响到整体的速度。 b.爬取过程的问题： 虽然该系统只是爬取用户的关注人，不爬取被关注人信息。所以不存在关注和被关注的关系，当是存在 多个人关注同一个人的情况，所以如果爬取所有每个人的关注人，仍然存在爬取了很多重复性爬过的过户，特别是爬取解析分页网页（某个用户关注人特别多的情况）；耗费资源时间却做很多无用功。 个人解决方法：控制爬取的分页数（实际爬去的分页数还需算法和大数据理论支撑），概率性爬取关注人的url（分页下）。存在多人关注同一人的关系便存在概率性爬取到此用户的可能。 缺点：部分概率爬取不到关注用户；爬取的用户数据不全。 grequets使用代理12proxies = &#123;&apos;http&apos;: &apos;http://219.148.108.126:8998&apos;&#125;s = (grequests.get(u, proxies=proxies) for u in urls) 参考 https://pypi.python.org/pypi/grequests https://github.com/kennethreitz/grequests http://blog.upeoe.com/2016/11/24/how-to-use-grequest/]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[python全局变量]]></title>
      <url>%2F2017%2F03%2F17%2Fpython%2F2017-03-17%2F</url>
      <content type="text"><![CDATA[python全局变量使用: 1 可以直接定义一个globalvar.py文件；里面存放其他文件要使用的变量。在其他文件中导入: 12345678910111213141516171819202122232425262728from src.globalvar import *# __author_=&quot;gLinlf&quot;# coding=utf-8import queue# 定义全局变量# 已经爬取得到所有urlhad_url = set()# 已经爬取解析用户的urlhad_used_url = set()# 用户关注的其他用户的url 使用队列（使用后删除）follow_url = queue.Queue(maxsize=0)# 爬虫入口# follow_url.put(&apos;https://www.zhihu.com/people/liaoxuefeng&apos;)follow_url.put(&apos;https://www.zhihu.com/people/competitionlaw&apos;)# 请求头header = &#123; &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36&apos;&#125;# 数据库名db_name = &apos;test&apos;# 抽样爬取不大于10个分页的用户(200人)（加快用户信息爬取速度！概率性减少 花费早爬取页面中存在较多已经爬去的用户url 的资源和时间。）max_page = int(10)# 异步IO请求解析的最大页码数max_parse_page = int(5) 2使用global关键字声明]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[mongodbengine 使用记录]]></title>
      <url>%2F2017%2F03%2F16%2Fmongodb%2F2017-03-16%2F</url>
      <content type="text"><![CDATA[mongoengine操作mongodb的使用笔记参见mongoengine文档 1 安装mongoengine pip install -U mongoengine 2 在python3.5中创建连接 mongoengine.connect（db = None，alias =’default’，** kwargs ） 2.1如果使用的时本地连接，则由’db’参数指定的数据库，直接连接数据库例如： mongoengine.connect(‘testdb’) 2.2如果数据库不在localhost的默认端口上运行，也可以在此处提供连接设置(如果需要验证，请提供用户名和密码参数。)： 1mongoengine.register_connection（alias，name = None，host = None，port = None，read_preference = Primary（），username = None，password = None，authentication_source = None，authentication_mechanism = None，** kwargs ） 参数： alias - 将用于在MongoEngine中引用此连接的名称 name - 要使用的特定数据库的名称 host - 要连接的mongod实例的主机名 port - mongod实例正在运行的端口 read_preference - 集合的读取首选项**添加pymongo 2.1 username - 要进行身份验证的用户名 密码 - 要验证的密码 authentication_source - 要进行身份验证的数据库 authentication_mechanism - 数据库认证机制。默认情况下，对MongoDB 3.0及更高版本使用SCRAM-SHA-1，对较旧的服务器使用MONGODB-CR（MongoDB Challenge Response协议）。 is_mock - 明确使用mongomock进行连接（也可以使用mongomock：//作为db主机前缀） kwargs - 要传递到pymongo驱动程序的特定参数，例如maxpoolsize，tz_aware等。有关完整列表，请参阅pymongo的MongoClient文档。 mongoengine在python中的基本使用：新增（save） 创建py文件testmongo.py 类为TestCategories ，TestCategories类似存入mongodb数据库中的表： import mongoengine class TestCategories(mongoengine.Document): # 用户信息地址 user_url = mongoengine.StringField() # 用户名 user_name = mongoengine.StringField() # 用户居住所在地 user_locations = mongoengine.ListField() 在另一个py文件创建一个测试文件 test.py: from src.testmogo import TestCategories from mongoengine import * connect(&apos;test&apos;) cate = TestCategories() cate.user_url = &apos;hello&apos; cate.user_name= &apos;python&apos; cate.user_locations = [&apos;llf&apos;] cate.save() 执行 test.py文件查看 数据库结果： 查询对于objects的到只是一个queryset类（QuerySet），这类里面有丰富的信息，存放所有查询到的 document对象（TestCategories）的list集合。可以使用for in list 语法将查询到的类对象里的数据灵活拼接成一个新的dict或list。 参考：Python中QuerySet和Objects类 obj = TestCategories.objects(user_url=&apos;123&apos;) #&lt;class &apos;mongoengine.queryset.queryset.QuerySet&apos;&gt; print(type(obj)) #&lt;TestCategories:TestCategoriesobject&gt; print(obj) for i in obj: print(i.user_url) 更新安装 django的web框架后 有update（）等方法。 obj = TestCategories.objects(user_url=&apos;123&apos;)获得的就是对应的document。 obj.user_url = &apos;321&apos; obj.update() 常规更新： obj = TestCategories.objects(user_url=&apos;123&apos;) #&lt;class &apos;mongoengine.queryset.queryset.QuerySet&apos;&gt; print(type(obj)) #[&lt;TestCategories: TestCategories object&gt;, &lt;TestCategories: TestCategories object&gt;] print(obj) for obj in objs: print(obj.user_url) # 直接操作 obj（TestCategories）对象 ，修改对应的字段值，更新！ obj.user_name = &apos;xx&apos; obj.save() 删除]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[How to use selenium]]></title>
      <url>%2F2017%2F03%2F13%2Fspider%2F2017-03-13%2F</url>
      <content type="text"><![CDATA[python 异步爬取库selenium使用记录简介 1 在做基于python的知乎用户信息特征爬虫系统中，遇到了个人信息需要点击 查看详细资料后 页面才能渲染出所有我所需要的用户信息。对于基于异步请求的动态网页爬虫，在研究了相关资料后发现这个暴力牛逼的包selenium，相对简单实用。简单来说就是模拟人为对浏览器的动作，比如：点击事件，填写表单等。这样就能克服操作我们传统爬虫只能爬取静态网页，得不到异步请求渲染后的网页信息。具体安装如下：安装selenium（python3.5，window10环境下） 1.通过pip安装 pip install selenium 2.通过下载selenium安装包 selenium连接https://pypi.python.org/pypi/selenium解压，cmd进入目录：E:\selenium\selenium2.53.5&gt; python3 setup.py install 安装浏览器驱动chromedriver（以google为例） 1.下载Chromedriver 下载地址：http://chromedriver.storage.googleapis.com/index.html 注意：先查看notes.txt，Chromedriver版本和浏览器版本一定要对应。否者会出现各种异常报错。 2.解压chromedriver.zip文件将里面的chromedriver.exe文件拷贝到谷歌浏览器的安装目录下： C:\Program Files (x86)\Google\Chrome\Application 3.配置环境变量 打开:我的电脑-&gt;属性-&gt;高级系统设置-&gt;环境变量 打开path在最后面添加 ;C:\Program Files (x86)\Google\Chrome\Application 测试 打开python编辑器 如pyCharm 编辑代码： chromedriver = “C:\Program Files (x86)\Google\Chrome\Application\chromedriver.exe”os.environ[“webdriver.chrome.driver”] = chromedriverdriver = webdriver.Chrome(chromedriver)driver.get(‘http://www.baidu.com‘) 如果跳出google浏览器，且为百度页面则安装成功。不知道为什么其他博客只需要： driver = webdriver.Chrome()driver.get(‘http://www.baidu.com‘) 便能成功。而且我环境变量也已经配置。不知道啥原因，有待研究下。 ———————————————————————————————selenium使用记录模拟知乎登录（具体使用查看文档） 引用链接：https://www.zhihu.com/question/46528604?sort=created ###a先实例化一个driver chrome_driver = “C:\Program Files (x86)\Google\Chrome\Application\chromedriver.exe” os.environ[&quot;webdriver.chrome.driver&quot;] = chrome_driver driver = webdriver.Chrome(chrome_driver) ###b 登录知乎，亲自测试可行： driver.get(“http://www.zhihu.com“) #打开知乎我们要登录time.sleep(2) #让操作稍微停一下driver.find_element_by_link_text(‘登录’).click() #找到‘登录’按钮并点击time.sleep(2) # 找到输入账号的框，并自动输入账号 这里要替换为你的登录账号driver.find_element_by_name(‘account’).send_keys(‘你的账号’)time.sleep(2)# 密码，这里要替换为你的密码driver.find_element_by_name(‘password’).send_keys(‘你的密码’)time.sleep(2)# 输入浏览器中显示的验证码，这里如果知乎让你找烦人的倒立汉字，手动登录一下，再停止程序，退出#浏览器，然后重新启动程序，直到让你输入验证码yanzhengma=input(‘验证码:’) #现在好像不需要验证码driver.find_element_by_name(‘captcha’).send_keys(yanzhengma) # 找到登录按钮，并点击driver.find_element_by_css_selector(‘div.button-wrapper.command &gt; button’).click() ###c 使用driver.page_source 可以获得实际操作后的页面源码。而requests库只能获得打开连接的静态网页。如：html = driver.page_source接下来就可以使用BeautifulSoup库来自由操作了。]]></content>
    </entry>

    
  
  
</search>
